# ğŸ§  CareBot Agent  
### Multi-Agent AI Assistant using AutoGen + Ollama + FastAPI

ğŸ”— **Repository:** https://github.com/Ujjwal0207/carebot-agent

CareBot Agent is a **production-style, multi-agent conversational AI system** built using **Microsoft AutoGen**, **FastAPI**, **WebSockets**, and **local LLMs via Ollama**.

This project demonstrates how real-world AI assistants are designed using **agent orchestration**, **intent routing**, **memory extraction**, and **safe LLM integration** â€” without relying on paid APIs.

---

## ğŸš€ What Is This Project?

CareBot is an **empathetic AI assistant** that can:

- Understand user intent (greeting, emotional support, planning, safety)
- Route messages intelligently
- Respond empathetically using a Care agent
- Generate structured guidance using planner logic
- Extract long-term memory automatically
- Run completely **locally** using Ollama
- Communicate in **real time** using WebSockets

This is **not a simple chatbot** â€” it is a **multi-agent AI system** designed with production constraints in mind.

---

## ğŸ—ï¸ High-Level Architecture

User (Browser UI)
        |
        | WebSocket
        v
FastAPI Server (web/server.py)
        |
        v
run_agent()  â”€â”€â–º Router
        |           |
        |           â”œâ”€â”€ Safety Handling
        |           â”œâ”€â”€ Care Mode
        |           â””â”€â”€ Planner Mode
        |
        v
RAG Context Builder (rag.py)
        |
        v
CareBot Agent (AutoGen + Ollama)
        |
        v
Memory Extractor Agent
        |
        v
Response â†’ WebSocket â†’ UI



## ğŸ¤– Agents in This System

### 1ï¸âƒ£ CareBot Agent
- Built with `ConversableAgent`
- Provides empathetic, human-like responses
- Uses system prompts to guide tone and behavior

### 2ï¸âƒ£ Memory Extractor Agent
- Automatically decides **what is worth remembering**
- Outputs structured JSON
- Stores long-term memory safely

> âš ï¸ AutoGen is used **correctly**:  
> `generate_reply()` is used for LLM calls (not agent-to-agent chat with Ollama).

---

## ğŸ”€ Intent Routing

Messages are routed before LLM invocation:

| Intent | Example |
|------|--------|
| `safety` | â€œI want to harm myselfâ€ |
| `care` | â€œI feel lost and overwhelmedâ€ |
| `planner` | â€œWhat should I do next?â€ |
| `greeting` | â€œHiâ€, â€œHelloâ€ |

This keeps responses safe, relevant, and predictable.

---

## ğŸ“š RAG (Retrieval-Augmented Generation)

- Past memory is retrieved when relevant
- Injected only into **system context**
- Prevents prompt leakage
- Reduces hallucinations
- Improves conversational continuity

---

## âš¡ Real-Time WebSocket UI

- Instant responses
- â€œğŸ¤– Thinkingâ€¦â€ indicator
- No page reloads
- Ready for token streaming upgrades

---

## ğŸ›  Tech Stack

| Layer | Technology |
|----|-----------|
Backend | FastAPI |
Real-time | WebSockets |
Agents | Microsoft AutoGen |
LLM | Ollama (Llama3) |
Language | Python 3.9+ |
Frontend | HTML + JavaScript |
Memory | JSON (extensible to FAISS) |

---

## ğŸ“‚ Project Structure

carebot-agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # Core orchestration
â”‚   â”œâ”€â”€ router.py                # Intent routing
â”‚   â”œâ”€â”€ rag.py                   # Context builder
â”‚   â”œâ”€â”€ memory.py                # Memory storage
â”‚   â”œâ”€â”€ agent_care.py
â”‚   â”œâ”€â”€ agent_memory_extractor.py
â”‚   â”œâ”€â”€ safety.py
â”‚   â””â”€â”€ tools.py
â”‚
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ server.py                # FastAPI + WebSocket
â”‚   â””â”€â”€ index.html               # UI
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ llm_config.py            # Ollama config
â”‚
â”œâ”€â”€ memory.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Ujjwal0207/carebot-agent.git
cd carebot-agent

2ï¸âƒ£ Create Virtual Environment
python3 -m venv .venv
source .venv/bin/activate     # macOS/Linux
.venv\Scripts\activate        # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ§  Install Ollama (Local LLM)

Download Ollama:
ğŸ‘‰ https://ollama.com

Pull a model:

ollama pull llama3


Keep Ollama running in the background.

âš™ï¸ LLM Configuration

config/llm_config.py

config_list = [
    {
        "model": "llama3",
        "base_url": "http://localhost:11434/v1",
        "api_key": "ollama"
    }
]

â–¶ï¸ Run the Application
uvicorn web.server:app --reload


Open in browser:
http://127.0.0.1:8000