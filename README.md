# ğŸ§  CareBot Agent  
### Multi-Agent AI Assistant using AutoGen + Ollama + FastAPI

ğŸ”— **Repository:** https://github.com/Ujjwal0207/carebot-agent

CareBot Agent is a **production-style, multi-agent conversational AI system** built using **Microsoft AutoGen**, **FastAPI**, **WebSockets**, and **local LLMs via Ollama**.

This project demonstrates how real-world AI assistants are designed using **agent orchestration**, **intent routing**, **memory extraction**, and **safe LLM integration** â€” without relying on paid APIs.

---

## ğŸš€ What Is This Project?

CareBot is an **empathetic AI assistant** that can:

- Understand user intent (greeting, emotional support, planning, safety)
- Route messages intelligently
- Respond empathetically using a Care agent
- Generate structured guidance using planner logic
- Extract long-term memory automatically
- Run completely **locally** using Ollama
- Communicate in **real time** using WebSockets

This is **not a simple chatbot** â€” it is a **multi-agent AI system** designed with production constraints in mind.

---

## ğŸ—ï¸ High-Level Architecture

flowchart TD
    U[User<br/>(Browser UI)] -->|WebSocket| WS[FastAPI Server<br/>web/server.py]

    WS --> ORCH[Agent Orchestrator<br/>app/main.py<br/>run_agent()]

    ORCH --> ROUTER[Intent Router<br/>app/router.py]

    ROUTER -->|Safety| SAFE[Safety Handler<br/>safety.py]
    ROUTER -->|Care| CARE[Care Mode]
    ROUTER -->|Planner| PLAN[Planner Mode]

    CARE --> RAG[RAG Context Builder<br/>rag.py]
    PLAN --> RAG

    RAG --> AGENT[CareBot Agent<br/>AutoGen + Ollama]

    AGENT --> MEM[Memory Extractor Agent<br/>JSON Output]

    MEM --> STORE[Long-Term Memory<br/>memory.json]

    AGENT -->|Response| WS
    WS -->|WebSocket| U


## ğŸ¤– Agents in This System

### 1ï¸âƒ£ CareBot Agent
- Built with `ConversableAgent`
- Provides empathetic, human-like responses
- Uses system prompts to guide tone and behavior

### 2ï¸âƒ£ Memory Extractor Agent
- Automatically decides **what is worth remembering**
- Outputs structured JSON
- Stores long-term memory safely

> âš ï¸ AutoGen is used **correctly**:  
> `generate_reply()` is used for LLM calls (not agent-to-agent chat with Ollama).

---

## ğŸ”€ Intent Routing

Messages are routed before LLM invocation:

| Intent | Example |
|------|--------|
| `safety` | â€œI want to harm myselfâ€ |
| `care` | â€œI feel lost and overwhelmedâ€ |
| `planner` | â€œWhat should I do next?â€ |
| `greeting` | â€œHiâ€, â€œHelloâ€ |

This keeps responses safe, relevant, and predictable.

---

## ğŸ“š RAG (Retrieval-Augmented Generation)

- Past memory is retrieved when relevant
- Injected only into **system context**
- Prevents prompt leakage
- Reduces hallucinations
- Improves conversational continuity

---

## âš¡ Real-Time WebSocket UI

- Instant responses
- â€œğŸ¤– Thinkingâ€¦â€ indicator
- No page reloads
- Ready for token streaming upgrades

---

## ğŸ›  Tech Stack

| Layer | Technology |
|----|-----------|
Backend | FastAPI |
Real-time | WebSockets |
Agents | Microsoft AutoGen |
LLM | Ollama (Llama3) |
Language | Python 3.9+ |
Frontend | HTML + JavaScript |
Memory | JSON (extensible to FAISS) |

---

## ğŸ“‚ Project Structure

carebot-agent/
â”‚
â”œâ”€â”€ app/                          # Core AI logic
â”‚   â”œâ”€â”€ main.py                   # Agent orchestration
â”‚   â”œâ”€â”€ router.py                 # Intent classification & routing
â”‚   â”œâ”€â”€ rag.py                    # Retrieval-Augmented Generation (RAG)
â”‚   â”œâ”€â”€ memory.py                 # Memory persistence layer
â”‚   â”œâ”€â”€ agent_care.py             # Empathetic CareBot agent
â”‚   â”œâ”€â”€ agent_memory_extractor.py # Long-term memory extraction agent
â”‚   â”œâ”€â”€ safety.py                 # Safety & crisis handling logic
â”‚   â””â”€â”€ tools.py                  # Shared utilities
â”‚
â”œâ”€â”€ web/                          # Web layer
â”‚   â”œâ”€â”€ server.py                 # FastAPI + WebSocket server
â”‚   â””â”€â”€ index.html                # Minimal real-time UI
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ llm_config.py             # Ollama / LLM configuration
â”‚
â”œâ”€â”€ memory.json                   # Persistent long-term memory
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md



âš™ï¸ Installation & Setup

1ï¸âƒ£ Clone the Repository

git clone https://github.com/Ujjwal0207/carebot-agent.git
cd carebot-agent

2ï¸âƒ£ Create Virtual Environment

python3 -m venv .venv

source .venv/bin/activate     # macOS/Linux
.venv\Scripts\activate        # Windows

3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

ğŸ§  Install Ollama (Local LLM)

Download Ollama:
ğŸ‘‰ https://ollama.com

Pull a model:

ollama pull llama3


Keep Ollama running in the background.

âš™ï¸ LLM Configuration

config/llm_config.py

config_list = [
    {
        "model": "llama3",
        "base_url": "http://localhost:11434/v1",
        "api_key": "ollama"
    }
]

â–¶ï¸ Run the Application:

uvicorn web.server:app --reload


â–¶ï¸ Open in browser:

http://127.0.0.1:8000